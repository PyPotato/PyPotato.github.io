[{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://pypotato.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://pypotato.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"《Kubernetes 源码剖析》笔记 Kubernetes 版本：1.14\n第 5 章：client-go 编程式交互 Client 客户端对象 client-go 支持 4 种 Client 客户端对象来与 Kubernetes API Server 交互：\n其中 RESTClient 是最基础的客户端。RESTClient 对 HTTP Request 进行了封装，实现了 RESTful 风格的 API。其他三种客户端都是基于 RESTClient 实现的。\nClientset： 是多个 Client 客户端的集合，只能处理 K8s 内置资源。 dynamicClient： 动态客户端，可以操作任意 k8s 资源，包括CRD资源。 kubeconfig 配置管理 kubeconfig 用于管理访问 kube-apiserver 的配置信息，同时它还支持访问多 kube-apiserver的配置管理，Kubernetes 的其他组件都使用 kubeconfig 配置信息来连接 kube-apiserver 组件。\nkubeconfig 种存储了集群、用户、命名空间和身份验证等信息，他通常包含 3 部分：\nclusters：定义集群信息，如 kube-apiserver 的服务地址及集群的证书信息等。 users：定义了集群用户身份验证的客户端凭据。 contexts：定义集群用户信息和命名空间等，用于将请求发送到指定的集群。 client-go 会读取 kubeconfig 配置信息，并生成 config 对象，用于与 kube-apiserver 通信。\n1 2 3 4 5 6 7 func main() { config, err := clientcmd.BuildConfigFromFlag(\u0026#34;\u0026#34;, clientcmd.RecommendedHomeFile) if err != nil { panic(err) } ... } RESTClient 客户端 1 2 3 4 5 6 7 8 9 10 11 12 func main() { ... restClient, err := rest.RESTClientFor(config) result := \u0026amp;corev1.PodList{} err = restClient.Get(). Namespace(\u0026#34;default\u0026#34;). Resource(\u0026#34;pods\u0026#34;). VersionedParams(\u0026amp;metav1.ListOptions{limit:500},scheme.ParameterCodec). Do(). Into(result) } 上面的代码用 config 对象实例化了 RESTClient 对象，进而构建 HTTP 请求参数。RESTClient 发送请求的过程对 Go 语言标准库 net/http 进行了封装，由 Do 函数实现：\n1 2 3 4 5 6 7 8 // file: staging/src/k8s.io/client-go/rest/request.go func (r *Request) Do(ctx context.Context) Result { var result Result err := r.request(ctx, func(req *http.Request, resp *http.Response) { result = r.transformResponse(resp, req) }) ... } ClientSet 客户端 RESTClient 作为一种最基础的客户端，在使用的时候需要指定 Resource 和 Version 等信息，编码时需要提前知道 Resource 所在的 Group 和对应的 Version 信息（具体实践可以在 b 站视频 中看到）。\n与之相比，ClientSet 使用起来更加便捷，它在 RESTClient 的基础上封装了对 Resource 和 Version 的管理方法。\n每一个 Resource 可以理解为一个客户端，而 ClientSet 是多个客户端的集合，每一个 Resource 和 Version 都以函数的方式暴露给开发者。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { ... clientSet, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } // 每一个 Resource 和 Version 都以函数的方式暴露给开发者 podClient := clientSet.CoreV1().Pods(apiv1.NamespaceDefault) list, err := podClient.List(metav1.ListOptions{limit:500}) if err != nil { panic(err) } } 对 Pod 资源对象执行 List 操作实际上是对 RESTClient 进行了封装：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // file: staging/src/k8s.io/client-go/kubernetes/typed/core/v1/pod.go func (c *pods) List(ctx context.Context, opts metav1.ListOptions) (result *v1.PodList, err error) { ... result = \u0026amp;v1.PodList{} err = c.client.Get(). Namespace(c.ns). Resource(\u0026#34;pods\u0026#34;). VersionedParams(\u0026amp;opts, scheme.ParameterCodec). Timeout(timeout). Do(ctx). Into(result) return } DynamicClient 客户端 DynamicClient 和 ClientSet 类似，也是对 RESTClient 的封装，不过它不仅可以对 Kubernetes 内置资源操作，还能对 CRD 操作。\n这是因为，ClientSet 需要预先实现每种 Resource 和 Version 的操作，其内部的数据都是结构化数据。而 DynamicClient 内部实现了 Unstructured，这使它能够处理用户自定义资源（无法提前预知数据结构）。\nDynamicClient 不是类型安全的，因此在访问 CRD 自定义资源时需要特别注意。例如，在操作指针不当的情况下会导致程序崩溃。\nClientSet 用于操作 Kubernetes 中的内建资源。这些内建资源都有严格的 API 规范，包括每个资源的字段类型和结构，在使用 clientset 时，开发人员只能操作这些已知的资源，并且这些资源的结构在编译时就已经确定了。因此不太可能出现类型不匹配的问题。而对于可以操作自定义资源的 DynamicClient，则不一定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 func main() { // 建立到 Kubernetes 集群的连接 config, err := rest.InClusterConfig() if err != nil { // 处理错误 fmt.Fprintf(os.Stderr, \u0026#34;Failed to get in-cluster config: %v\\n\u0026#34;, err) return } // 创建一个 DynamicClient dynamicClient, err := dynamic.NewForConfig(config) if err != nil { // 处理错误 fmt.Fprintf(os.Stderr, \u0026#34;Failed to create DynamicClient: %v\\n\u0026#34;, err) return } // 定义自定义资源对象的 GVK（Group-Version-Kind） gvk := schema.GroupVersionKind{ Group: \u0026#34;example.com\u0026#34;, Version: \u0026#34;v1\u0026#34;, Kind: \u0026#34;CustomResource\u0026#34;, } // 创建一个自定义资源对象 resource := schema.GroupVersionResource{ Group: \u0026#34;example.com\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;customresources\u0026#34;, } // 创建一个错误的 CustomResource 实例，没有设置 spec 字段 obj := map[string]interface{}{ \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;CustomResource\u0026#34;, \u0026#34;metadata\u0026#34;: map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;example-cr\u0026#34;, }, \u0026#34;status\u0026#34;: map[string]interface{}{ \u0026#34;field\u0026#34;: \u0026#34;value\u0026#34;, }, } // 在对象上调用 Create 方法，这可能导致类型不安全而崩溃 _, err = dynamicClient.Resource(resource).Namespace(\u0026#34;default\u0026#34;).Create(context.TODO(), obj, metav1.CreateOptions{}) if err != nil { // 处理错误 fmt.Fprintf(os.Stderr, \u0026#34;Failed to create custom resource: %v\\n\u0026#34;, err) return } } DynamicClient 的处理过程将 Resource（如 PodList）转换成 Unstructured 结构类型，处理完成后，再将 unstructured 转换成 PodList。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func main() { ... dynamicClient, err := dynamic.NewForConfig(config) if err != nil { panic(err) } gvr := schema.GroupVersionResource{Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;pods\u0026#34;} unstructObj, err := dynamicClient.Resource(gvr). // 设置资源组、资源版本、资源名称 Namespace(apiv1.NamespaceDefault). List(metav1.ListOptions{limit: 500}) // 得到的 Pod 列表为 unstructured.UnstructuredList 指针类型 if err != nil { panic(err) } podList := \u0026amp;corev1.PodList{} err = runtime.DefaultUnstructuredConvert.FromUnstructured(unstructObj.UnstructuredContent(), podList) if err != nil { panic(err) } } DiscoveryClient 客户端 DiscoveryClient 是发现客户端，它主要用于发现 Kubernetes API Server 所支持的资源组、资源版本、资源信息。Kubernetes API Server 支持很多资源组、资源版本、资源信息，开发者在开发过程中很难记住所有信息，此时可以通过 DiscoveryClient 查看所支持的资源组、资源版本、资源信息。\nDiscoveryClient 除了可以发现 Kubernetes API Server 所支持的资源组、资源版本、资源信息，还可以将这些信息存储到本地，用于本地缓存（Cache），以减轻对 Kubernetes API Server 访问的压力。\nInformer 机制 Kubernetes 各个组件都是通过 REST API跟API Server 交互通信的，而如果每次每一个组件都直接跟 API Server 交互去读取/写入到后端的 etcd 的话，会对 API Server 以及 etcd 造成非常大的负担。而 Informer 机制是为了保证各个组件之间通信的实时性、可靠性，并且减缓对 API Server和etcd 的负担。\nInformer 机制架构设计 每一个 Kubernetes 资源上都实现了 Informer 机制。每一个 Informer 上都会实现 Informer 和 Lister 方法，定义不同资源的 Informer 可以监控不同资源的资源事件：\n1 2 podInformer := sharedInformer.Core().V1().Pods().Informer() nodeInformer := sharedInformer.Node().V1beta1().RuntimeClasses().Informer() Informer 也称作 Shared Informer，它是可以共享使用的。如果同一资源的 informer 被多次实例化，且每个 Informer 都读独立使用一个 Reflector，那么会运行过多相同的 ListAndWatch，导致 API Server 负载过重。\nShared Informer 可以使同一类资源 Informer 共享一个 Reflector，这样可以节约很多资源。通过 map 数据结构实现共享的 Informer 机制。\n1 2 3 4 type sharedInformerFactory struct { ... informers map[reflect.Type] cache.SharedIndexInformer } InformerFor 函数可以添加不同资源的 Informer，如果已经存在同类型的资源 Informer，则返回当前 Informer。\nReflector Reflector 负责与 apiserver 建立连接，使用 ListAndWatch 的方法获取资源对象的增量更新。\nReflector 源码实现中最主要的是 ListerAndWatch 函数，它可以分成两部分：获取资源列表数据和监控资源对象。\n1. 获取资源列表数据\nList 流程：\n首次连接时会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化 2. 监控资源对象\n若监听中途出现异常，Reflector 在重连时会携带上个版本的 rsourceVersion，并从该 resourceVersion 增量更新，避免再次全量 List。 一旦该对象的实例有 Add、Delete、Update 动作，Reflector 都会收到 “事件通知”，这时，该事件与对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。 DeltaFIFO DeltaFIFO负责管理资源对象的增量变化通知和提供增量同步的功能。\nDeltaFIFO 可以分开理解：\nFIFO 是一个先进先出的队列。 Delta 是一个资源对象存储，可以保存资源对象的操作类型。 1 2 3 4 5 6 7 type DeltaFIFO struct { ... items map[string] Deltas queue []string // 存储资源对象的 key，通过 `KeyOf` 函数计算得到 ... } type Deltas []Delta DeltaFIFO 与其他队列最大的不同之处是，它会保留所有关于资源对象（obj）的操作类型，队列中会存在拥有不同操作类型的同一个资源对象，消费者在处理该资源对象时能够了解该资源对象所发生的事情。\nDeltaFIFO 本质上是个先进先出队列，有生产者和消费者，生产者是 Reflector 调用的 Add 方法，消费者是 Controller 调用的 Pop 方法。\n1. 生产者方法\nDeltaFIFO 队列中的资源对象在 Added 事件、Updated 事件、Delete 事件中都调用了 queueActionLocked 函数：\n首先通过 KeyOf 函数计算出资源对象的 Key。 将 actionType 和资源对象构造成 Delta，添加到 items 中，并去重。 更新构造后的 Delta 并通过 cond.Broadcast 通知所有消费者解除阻塞。 2. 消费者方法\nPop 方法作为消费者方法使用，它的入参 process 是用于接收并处理对象的回调方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { //阻塞 直到调用了f.cond.Broadcast() f.cond.Wait() } //取出第一个元素 id := f.queue[0] f.queue = f.queue[1:] ... item, ok := f.items[id] ... delete(f.items, id) //这个process可以在controller.go中的processLoop()找到 //初始化是在shared_informer.go的Run //最终执行到shared_informer.go的HandleDeltas方法 err := process(item) //如果处理出错了重新放回队列中 if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } ... } } 当队列中没有数据时，通过 f.cond.wait 阻塞等待数据，直到收到 cond.Broadcast 才解除阻塞。 如果队列不为空，取队头数据，将该对象传入 process 函数，由上层消费者进行处理。若 process 回调函数处理出错，则重新入队。 所以整体流程如图：\n将对象存储至 Indexer 后，将资源对象分发到 SharedInformer 对应的事件处理函数，在 Informer 中我们通过 informer.AddEventHandler 添加了对资源事件的处理函数。可以看个例子\n3. Resync 机制\n为什么需要 Resync？\n首先回顾 Informer 的架构图，一共有三方需要同步数据\n首先是 apiserver 与 Informer 之间通过 resourceVersion 保证数据的一致性。 然后是 Informer 和 controller 之间：协程将 FIFO 队列中的事件取出更新至 Indexer 后，还会将事件同步回调至 custom controller，加入到 workqueue 队列中。 但是在处理 Informer 事件回调时，并不关心其返回的结果:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // Informer 的 Pop 方法的回调函数最后会调用到这个函数 func processDeltas( // Object which receives event notifications from the given deltas handler ResourceEventHandler, clientState Store, deltas Deltas, isInInitialList bool, ) error { // from oldest to newest for _, d := range deltas { obj := d.Object switch d.Type { case Sync, Replaced, Added, Updated: //查一下是否在Indexer缓存中 如果在缓存中就更新缓存中的对象 if old, exists, err := clientState.Get(obj); err == nil \u0026amp;\u0026amp; exists { if err := clientState.Update(obj); err != nil { return err } handler.OnUpdate(old, obj) } else { //没有在Indexer缓存中 把对象插入到缓存中 if err := clientState.Add(obj); err != nil { return err } handler.OnAdd(obj, isInInitialList) } ... } } return nil } 当出现处理失败的情况时，定时的 Resync 让这些处理失败的事件有了重新 onUpdate 处理的机会\nResync 做了什么？\nResync 的逻辑非常简单，就是定时将本地缓存（Indexer）中所有的资源对象重新推送到 DeltaFIFO 中，并设定操作类型为 Sync。\n4. SharedInformer 消费 Deltas\n主要看回调函数 processDeltas（见👆）\nIndexer Indexer 负责存储资源对象的完整副本和提供查询功能，用于减轻 API Server 和 Etcd 的压力。\nIndexer 是在 ThreadSafeMap 的基础上进行了封装，它继承了 ThreadSafeMap 的相关方法，并实现了 Indexer Func 等功能 —— 提供了索引功能。\n1. ThreadSafeMap 并发安全存储\nTreadSafeMap 结构如下：\n1 2 3 4 type threadSafeMap struct { item map[string] interface{} ... } item 的 key 通过 keyFunc 函数计算，默认使用 MetaNamespaceKeyFunc 函数计算出 \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 格式的 key，value 用于存储资源对象。\n2. Indexer 索引器\nIndexer 可以自定义索引函数，它有 4 个重要数据结构：Indices、Index、Indexers 和 IndexFunc。\n1 2 3 4 5 6 7 type Indexers map[string]IndexFunc type IndexFunc func(obj interface{}) ([]string, error) type indices map[string]Index type index map[string]sets.String Indexers：存储索引器，key 为索引器名称，value 为索引器的实现函数。 IndexFunc：索引器函数，定义为接收一个资源对象，返回检索结果列表。 Indices：存储缓存器，key 为缓存器名称，value 为缓存数据。 Index：存储缓存数据，其结构为 K/V。 可以先来看一个自定义索引的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package main // 导入包 import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/tools/cache\u0026#34; ) // 定义一个函数，用于根据 Pod 对象的 annotations 字段中的 users 属性，生成一个包含所有用户名的字符串切片 func UsersIndexFunc(obj interface{}) ([]string, error) { // 获取 Pod 对象 pod := obj.(*metav1.Pod) // 获取 users 属性 usersString := pod.Annotations[\u0026#34;users\u0026#34;] // 将 users 属性分割为字符串切片 return strings.Split(usersString, \u0026#34;,\u0026#34;), nil } // 主函数 func main() { // 根据 UsersIndexFunc 建立相应的索引结构 index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{\u0026#34;byUser\u0026#34;: UsersIndexFunc}) // 创建三个 Pod 对象 pod1 := \u0026amp;metav1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;one\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;ernie,bert\u0026#34;}}} pod2 := \u0026amp;metav1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;two\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;bert,oscar\u0026#34;}}} pod3 := \u0026amp;metav1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;tre\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;ernie,elmo\u0026#34;}}} // 将 Pod 对象添加到 cache.Indexer 对象中 index.Add(pod1) index.Add(pod2) index.Add(pod3) // 从刚刚构建的索引结构中，获取所有包含用户名为 \u0026#34;ernie\u0026#34; 的 Pod 对象 erniePods, err := index.ByIndex(\u0026#34;byUser\u0026#34;, \u0026#34;ernie\u0026#34;) if err != nil { panic(err) } // 遍历所有包含用户名为 \u0026#34;ernie\u0026#34; 的 Pod 对象，并打印其名称 for _, erniePod := range erniePods { fmt.Println(erniePod.(*metav1.Pod).Name) } } // 输出 one tre 上面的代码首先定义了一个索引器函数 UserIndexFunc，在该函数中，我们定义查询出所有 Pod 资源下 Annotations 字段的 key 为 users 的 Pod。\ncache.NewIndexer 函数实例化了 Indexer 对象，它接收两个参数：第一个是 KeyFunc，用于计算资源对象的 key；第二个是 cache.Indexers，用于定义索引器，其中 key 为索引器的名称（byUser），value 为索引器。通过 index.Add 函数添加 3 个 Pod 资源对象。最后通过 index.ByIndex 函数查询 byUser 索引器下匹配 ernie 字段的 Pod 列表。\n所以 Indexer 中的 4 个结构之间的协同关系如下：\n根据自定义索引函数建立索引结构 通过 Add 方法将资源对象加入这个索引结构 利用该索引结构提供的索引方法获取满足条件的资源对象的 key 用这个 key 到真正存储的地方获取资源对象 3. 索引器核心实现\n1 2 3 4 5 6 7 8 9 10 func (c *threadSafeMap) ByIndex(indexName, indexKey string) ([]interface{}, error) { indexFunc := c.indexers[indexName] // 查找索引名对应的索引函数 index := c.indices[indexName] // 查找索引名对应的缓存器 set := index[indexKey] // 从 index 中获取对应的数据 list := make([]interface{}, 0, set.Len()) for key := range set.List() { list = append(list, c.items[key]) } return list, nil } Index 缓存数据为集合（元素不重复）。由于 Go 标准库没有提供 Set 数据结构，而 Go 中 map 是不能存在相同的 key 的，因此 Kubernetes 将 map 结构类型的 key 作为 Set 数据结构，实现去重特性。\nWorkQueue Event 产生的速度比处理 Event 的速度快，WorkQueue就是为了解决这种速度不一致的问题而引入的\nWorkQueue 相比普通的 FIFO 相比支持：\n有序：按添加顺序处理元素。 去重：相同元素在同一时间不会被重复处理。 并发性：多生产者、消费者。 标记机制：支持标记，标记一个元素是否被处理，允许元素在处理时重新排队。 通知机制：ShutDown 方法通过信号量通知队列不再接收新元素。 延迟：支持延迟队列，延迟一段时间后再进将元素入队。 限速：支持限速队列，限制一个元素被重新排队的速率。 Metric：支持 metric 监控指标，用于 Prometheus 监控。 WorkQueue 支持 3 种队列，提供 3 种接口：\nInterface：FIFO 队列接口，支持去重。 DelayInterface：延迟队列接口，基于 Interface 封装。 RatelimitingInterface：限速队列接口，基于 DelayInterface 封装。 FIFO 队列 方法说明：\n1 2 3 4 5 6 7 8 type Interface interface { Add(item interface{}) // 给队列添加元素，类型任意 Len() int Get() (item interface{}, shutdown bool) // 获取队头元素 Done(item interface{}) // 标记元素已被处理 ShutDown() // 关闭队列 ShuttingDown() bool // 查询队列是否正在被关闭 } FIFO 数据结构定义：\n1 2 3 4 5 6 7 8 9 10 11 type Type struct { queue []t // 实际存储元素的地方 dirty set // 保证去重，并且保证同一个元素被并发添加多次，也只会被处理一次 processing set // 标记一个元素是否被正常处理 cond *sync.Cond shuttingDown bool metrics queueMetrics unfinishedWorkUpdatePeriod time.Duration clock clock.Clock } 存储过程如图：\n通过 Add 方法往 FIFO 中插入三个元素。 通过 Get 方法获取最先进入的元素，放入 processing 队列。 通过 Done 方法标记该元素已经被处理完成，并从 processing 队列中出队。 FIFO 并发存储过程：\ngoroutine A 通过 Get 获取队列 1 元素，将其添加到 processing 队列。 同一时间，goroutine B 通过 Add 方法插入另一个 1 元素。 此时由于在 processing 队列已经存在相同的元素，所以后面的 1 元素不会直接添加到 queue 字段中，而是添加到 dirty。 当 goroutine A 通过 Done 标记处理完成后，如果 dirty 字段中有 1 元素，则将其追加到 queue 字段尾部。 延迟队列 延迟队列基于 FIFO 队列接口，新增了 AddAfter 方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type DelayingInterface interface { Interface AddAfter(item interface{}, duration time.Duration) } type delayingType struct { Interface clock clock.Clock stopCh chan struct{} heartbeat clock.Ticker waitingForAddCh chan *waitFor // waitchan 默认大小 1000 metrics retryMetrics deprecatedMetrics retryMetrics } type waitFor struct { data t // 准备添加到队列中的数据 readyAt time.Time // 应该被加入队列的时间 index int // 在 waitForPriorityQueue（小顶堆）中的索引 } 其中最主要的字段是 waitingForAddCh ，默认大小 1000。\n延迟队列通过 AddAfter 方法插入元素，当插入的元素大小大于等于 1000 时，延迟队列阻塞。运行原理：\n通过 AddAfter 将元素加入 waitingForAddCh，waitingLoop 来消费元素。当元素的延迟时间未到达时，则将元素放入按延迟时间排序的优先队列（waitForPriorityQueue）中。当元素延迟时间到达时，将元素插入 FIFO 队列，并遍历优先队列，按上述逻辑验证时间。\n限速队列 基于延迟队列和 FIFO 队列接口封装，在原有的功能上增加了 AddRateLimited、Forget、NumRequeues 方法。\n限速队列的重点在于它提供的 4 种限速算法接口（RateLimiter）。它的原理是利用延迟队列的特性，延迟某个元素的插入时间，达到限速目的。\n限速队列接口方法：\n1 2 3 4 5 type RateLimiter interface { When(item interface{}) time.Duration // 获取指定元素应该等待的时间 Forget(item interface{}) // 释放指定元素，清空该元素的排队数 NumRequeues(item interface{}) int // 获取指定元素的排队数 } 引入一个概念 —— 限速周期：从执行 AddRateLimited 方法到执行完 Forget 方法之间的时间。\n1. 令牌桶算法\n通过 Go 三方库 golang.org/x/time/rate 实现。令牌桶算法内部实现了一个存放 token 的桶，初始桶为空，token 会以固定速率往桶里填充，直到填满，多余的 token 会被丢弃。每个元素都会从令牌桶中得到一个 token，只有得到 token 的元素才能被 accept，否则处于等待状态。即通过控制发放 token 的速率达到限速目的。\n2. 排队指数算法\n排队指数算法将相同元素的排队数作为指数，排队数增大，速率限制呈指数级增长，但其最大不会超过 maxDelay。核心实现：\n1 2 3 4 5 6 exp := r.failures[item] r.failures[item] = r.failures[item] + 1 backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) if backoff \u0026gt; math.MaxInt64 { return r.maxDelay } 其中，failures 用于统计元素排队数，每当 AddRateLimited 插入新元素，该字段加 1；baseDelay 是最初限速（系数，默认 5 ms），maxDelay 是最大限速单位（默认 1000 s）。 延迟时间的计算公式可以总结为：$back off = baseDeley * 2^{\\exp}$，其中 exp 为排队数。\n3. 计数器算法\n通过 限制一段时间内允许通过元素数 达到限速目的。每插入一个元素，计数器加 1，当计数器到达阈值并且还在限速周期内时，则不允许元素通过。WorkQueue 在此基础上扩展了两种延迟速率：\n1 2 3 4 5 r.failures[item] = r.failures[item] + 1 if r.failures[item] \u0026lt;= r.maxFastAttempts { return fastDelay } return slowDelay 4. 混合模式\n将多种限速算法混合使用，如同时使用排队指数算法和令牌桶算法：\n1 2 3 4 5 6 func DefaultControllerRateLimiter() RateLimiter { return NewMaxOfRateLimiter( NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second), \u0026amp;BucketRateLimiter{limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } EventBroadcaster 事件管理器 这里的 Event 是 Kubernetes 所管理的 Event 资源对象，而非 Etcd 集群监控机制产生的回调事件，注意区分。\nEvent 作为一种集群资源保存在 Etcd 中，为了避免磁盘空间被填满，只保留最近 1 小时的事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Event 数据结构定义 type Event struct { metav1.TypeMeta metav1.ObjectMeta InvolvedObject ObjectReference Reason string Message string Source EventSource FirstTimestamp metav1.Time LastTimestamp metav1.Time Count int32 Type string EventTime metav1.MicroTime Series *EventSeries Action string Related *ObjectReference ReportingController string ReportingInstance string } EventBroadcaster 事件管理机制原理图：\n在定位上，可以把 EventBroadcaster 理解成集成在各个组件中的一套 SDK，它可以用来产生和分发事件。其中主要有三个组件：\nEventRecorder：事件生产者。Kubernetes 组件通过 EventRecorder 记录关键性事件。 EventBroadcaster：事件消费者。消费 EventRecorder 记录的事件并分发给所有已连接的 broadcasterWatcher。有非阻塞分发和阻塞分发机制。 broadcasterWatcher：用于定义事件的处理方式，如上报到 API Server。 1. EventRecorder\nEventRecorder 接口方法以及使用示例如下：\n1 2 3 4 5 6 7 8 9 10 11 type EventRecorder interface { Event(...) // 记录刚发生的事件 Eventf(...) // 格式化输出事件的格式 PastEventf(...) // 允许自定义事件发生的事件，以记录已发生过的消息 AnnotatedEventf(...) // 功能与 Eventf 一样，添加了 Annotation 字段 } // Event 最终会调用到 Action 函数异步地将事件写入 m.incoming Chan。 func (m *Broadcaster) Action(action EventType, obj runtime.Object) { m.incoming \u0026lt;- Event{action, obj} } 2. EventBroadcaster\n1 2 3 4 // EventBroadcaster 实例化示例 func NewBroadcaster() EventBroadcaster { return \u0026amp;eventBroadcasterImpl{watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration} } watch.NewBroadcaster 会在函数内部启动 goroutine 监控 m.incoming，并将监控的事件通过 m.distribute 函数分发给所有已连接的 broadcasterWatcher。\n分发有两种机制：非阻塞分发机制 和 阻塞分发机制。其中，非阻塞分发用 DropIfChannelFull 标识，阻塞分发用 WaitIfChannelFull 标识，默认为非阻塞，示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (m *Broadcaster) distribute(event Event) { m.lock.Lock() defer m.lock.Unlock() if m.fullChannelBehavior == DropIfChannelFull { for _, w := range m.watchers { select { case w.result \u0026lt;- event: case \u0026lt;-w.stopped: default: } } } else { for _, w := range m.watchers { select { case w.result \u0026lt;- event: case \u0026lt;-w.stopped: } } } } 非阻塞使用 select 中的 default 关键字实现，当缓冲区满时，事件丢失。而阻塞方式在缓冲区满时会选择等待。\nKubernetes 中的事件资源与其他资源不同的是它可以丢失。\n3. broadcasterWatcher\nbroadcasterWatcher 是每个系统组件自定义处理事件的方式。每个 broadcasterWatcher 有两种自定义处理事件的函数：\nStartLogging：将事件写入日志。 StartRecordingToSink：将事件上报到 API Server 并存到 Etcd。 上面这两个函数都依赖于 StartEventWatcher 函数，其内部跑了一个 goroutine，不断监控 EventBroadcaster 来发现事件并调用相关函数对事件处理。\n以 kube-scheduler 组件为例：\n1 2 3 4 5 if cc.Broadcaster != nil \u0026amp;\u0026amp; cc.EventClient != nil { cc.Broadcaster.StartLogging(klog.V(6).Info()) cc.Broadcaster.StartRecordingToSink(\u0026amp;v1core. EventSinkImpl{Interface: cc.EventClient.Events(\u0026#34;\u0026#34;)}) } 它将 v1core.EventSinkImpl 作为上报事件的自定义函数。上报事件有 3 种方法：Create（Post 方法）、Update（Put 方法）、Patch（Patch 方法）。以 Create 为例，Create =\u0026gt; e.Interface.CreateWithEventNamespace：\n1 2 3 4 5 6 7 8 9 func (e *events) CreateWithEventNamespace(event *v1.Event) (*v1.Event, error) { ... result := \u0026amp;v1.Event{} err := e.client.Post(). NamespaceIfScoped(event.Namespace, len(event.Namespace) \u0026gt; 0). Resource(\u0026#34;events\u0026#34;). Body(event). Do(). } 上报过程通过 RESTClient 发送 Post 请求，将事件发送到 API Server，最终存在 Etcd。\n","date":"0001-01-01T00:00:00Z","permalink":"https://pypotato.github.io/p/","title":""},{"content":"This is a test page ","date":"0001-01-01T00:00:00Z","permalink":"https://pypotato.github.io/p/","title":""}]